{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602de56f-8d7c-4edb-9560-5c69c535bdf4",
   "metadata": {},
   "source": [
    "# Homework - Module 03\n",
    "\n",
    "The goal of this homework is to create a simple training pipeline, use mlflow to track experiments and register best model, but use an orchestration tool for it.\n",
    "\n",
    "We'll use [the same NYC taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page), the **Yellow** taxi data for March, 2023. \n",
    "\n",
    "### Question 1. Select the Tool\n",
    "\n",
    "We will use the same tool you used when completing the module: `Prefect`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f41f0-a1ca-410d-8887-c4a11fcd3d66",
   "metadata": {},
   "source": [
    "### Question 2. Version\n",
    "\n",
    "The version of our orchestrator is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c0f7a1-fc4d-4965-8dfc-338dbc58e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.6\n"
     ]
    }
   ],
   "source": [
    "# Prefect version\n",
    "!prefect --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b1de4-1aeb-43b5-852a-dc18a56889c6",
   "metadata": {},
   "source": [
    "### Question 3. Creating a pipeline\n",
    "\n",
    "Let's download and read the March 2023 Yellow taxi trips data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d05b495-7d02-48a4-ac40-aa0a705e1b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 53.5M  100 53.5M    0     0  1197k      0  0:00:45  0:00:45 --:--:-- 1183k\n"
     ]
    }
   ],
   "source": [
    "# Download March data\n",
    "!curl -O https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e94bd1-460d-4bc2-8232-230c68a58929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 cm-mboulou-mac  staff    54M Jun 13 06:37 yellow_tripdata_2023-03.parquet\n"
     ]
    }
   ],
   "source": [
    "# List data file\n",
    "!ls -lh yellow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1b4d7f-68f8-4ef8-a776-11c49582e6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.18\n"
     ]
    }
   ],
   "source": [
    "# Necessary import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Python version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3335483-c2c8-4ea7-aec2-b5f267dd6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3403766.\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "df = pd.read_parquet(\"yellow_tripdata_2023-03.parquet\")\n",
    "# Number of observations\n",
    "print(f\"Number of rows: {len(df)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be86fe5-de20-4983-a1b2-b002a19d811e",
   "metadata": {},
   "source": [
    "\n",
    "### Question 4. Data preparation\n",
    "\n",
    "Let's continue with pipeline creation.\n",
    "\n",
    "We will use the same logic for preparing the data we used previously. \n",
    "\n",
    "This is what we used (adjusted for yellow dataset):\n",
    "\n",
    "```python\n",
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df.duration = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "Let's apply to the data we loaded in question 3. \n",
    "\n",
    "What's the size of the result? \n",
    "\n",
    "- 2,903,766\n",
    "- 3,103,766\n",
    "- 3,316,216 \n",
    "- 3,503,766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5cedb89-fc9a-470c-b4e9-49c40de3ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading the data\n",
    "def read_dataframe(filename):\n",
    "    # Read the parquet data file\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    # Feature engineering for creating the duration column\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    # Convert durations into minutes\n",
    "    df.duration = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    # Filtering for trips lasting from 1 min to 1 hour\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    # Categorical features\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    # Convert categorival variables to string\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    # return the data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf466752-414e-4fb6-9d3e-7db9b815785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3316216.\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "df = read_dataframe(\"yellow_tripdata_2023-03.parquet\")\n",
    "# Number of observations\n",
    "print(f\"Number of rows: {len(df)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d1f79-8408-4b56-8fc9-c5d349f667a1",
   "metadata": {},
   "source": [
    "### Question 5. Train a model\n",
    "\n",
    "We will now train a linear regression model using the same code as in homework 1.\n",
    "\n",
    "* Fit a dict vectorizer.\n",
    "* Train a linear regression with default parameters.\n",
    "* Use pick up and drop off locations separately, don't create a combination feature.\n",
    "\n",
    "Let's now use it in the pipeline. We will need to create another transformation block, and return both the dict vectorizer and the model.\n",
    "\n",
    "What's the intercept of the model? \n",
    "\n",
    "Hint: print the `intercept_` field in the code block\n",
    "\n",
    "- 21.77\n",
    "- 24.77\n",
    "- 27.77\n",
    "- 31.77"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a0059-91f7-47fa-9816-7a72f1e5504b",
   "metadata": {},
   "source": [
    "### Question 6. Register the model \n",
    "\n",
    "The model is trained, so let's save it with MLFlow.\n",
    "\n",
    "Find the logged model, and find MLModel file. What's the size of the model? (`model_size_bytes` field):\n",
    "\n",
    "* 14,534\n",
    "* 9,534\n",
    "* 4,534\n",
    "* 1,534"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b6a55-6f22-44d6-b5ff-962e6f540f9a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
