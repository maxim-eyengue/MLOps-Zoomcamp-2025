{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602de56f-8d7c-4edb-9560-5c69c535bdf4",
   "metadata": {},
   "source": [
    "# Homework - Module 04\n",
    "\n",
    "In this homework, we'll deploy the ride duration model in batch mode. Like in homework 1, we'll use [the NYC taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page), more specifically the **Yellow** Yellow Taxi Trip Records dataset.\n",
    "\n",
    "### Question 1. Notebook\n",
    "\n",
    "We'll start with the same notebook we ended up with in homework 1. We cleaned it a little bit and kept only the scoring part. You can find the initial notebook [here](./starter.ipynb).\n",
    "\n",
    "After, running this notebook for the March 2023 data, the standard deviation of the predicted duration for this dataset is `1.24 6.24 12.28 18.28`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f41f0-a1ca-410d-8887-c4a11fcd3d66",
   "metadata": {},
   "source": [
    "### Question 2. Preparing the output\n",
    "\n",
    "Like in the course videos, we want to prepare the dataframe with the output. \n",
    "\n",
    "First, let's create an artificial `ride_id` column:\n",
    "\n",
    "```python\n",
    "df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "```\n",
    "\n",
    "We can then write the ride id and the predictions to a dataframe with results, saving it as parquet:\n",
    "\n",
    "```python\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine = 'pyarrow',\n",
    "    compression = None,\n",
    "    index = False\n",
    ")\n",
    "```\n",
    "\n",
    "__Note:__ We used the snippet above for saving the file. It should contain only these two columns. For this question, we didn't change the\n",
    "dtypes of the columns and used `pyarrow`, not `fastparquet`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file size\n",
    "!ls lh ouput_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e70d43",
   "metadata": {},
   "source": [
    "The size of the output file is ` 36M 46M 56M 66M`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b1de4-1aeb-43b5-852a-dc18a56889c6",
   "metadata": {},
   "source": [
    "### Question 3. Creating the scoring script\n",
    "\n",
    "Now let's execute a command to turn the notebook into a script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34490838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn notebook to script\n",
    "!jupyter nbconvert --to=script starter.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be86fe5-de20-4983-a1b2-b002a19d811e",
   "metadata": {},
   "source": [
    "\n",
    "### Question 4. Virtual environment\n",
    "\n",
    "Now let's put everything into a virtual environment. We'll use pipenv for that.\n",
    "\n",
    "We first install all the required libraries, paying attention to the Scikit-Learn version (same version as in the starter notebook).\n",
    "\n",
    "After installing the libraries, pipenv creates two files: `Pipfile`\n",
    "and `Pipfile.lock`. The `Pipfile.lock` file keeps the hashes of the\n",
    "dependencies we use for the virtual env.\n",
    "\n",
    "The first hash for the Scikit-Learn dependency is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d1f79-8408-4b56-8fc9-c5d349f667a1",
   "metadata": {},
   "source": [
    "### Question 5. Parametrize the script\n",
    "\n",
    "Let's now make the script configurable via CLI. We'll create two \n",
    "parameters: year and month, and run the script for April 2023, adding a print statement to it. \n",
    "\n",
    "The mean predicted duration is ` 7.29 14.29 21.29 28.29`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a0059-91f7-47fa-9816-7a72f1e5504b",
   "metadata": {},
   "source": [
    "### Question 6. Docker container \n",
    "\n",
    "Finally, we'll package the script in the docker container. For that, we'll need to use a base image already prepared. This is what the content of this image is:\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.10.13-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY [ \"model2.bin\", \"model.bin\" ]\n",
    "```\n",
    "\n",
    "Note: There is no need to run it. It was already done to be pushed to [`agrigorev/zoomcamp-model:mlops-2024-3.10.13-slim`](https://hub.docker.com/layers/agrigorev/zoomcamp-model/mlops-2024-3.10.13-slim/images/sha256-f54535b73a8c3ef91967d5588de57d4e251b22addcbbfb6e71304a91c1c7027f?context=repo), which we will use as base image.\n",
    "\n",
    "Our Dockerfile is as follows:\n",
    "\n",
    "```dockerfile\n",
    "FROM agrigorev/zoomcamp-model:mlops-2024-3.10.13-slim\n",
    "\n",
    "# do stuff here\n",
    "```\n",
    "\n",
    "This image already has a pickle file with a dictionary vectorizer\n",
    "and a model. We will use them. Indeed, There is no need to copy the model to the docker image, but only to use the pickle file already in the image. \n",
    "\n",
    "Now let's run the script with docker:\n",
    "\n",
    "The mean predicted duration\n",
    "for May 2023 is `0.19 7.24 14.24 21.19`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c10c1a",
   "metadata": {},
   "source": [
    "#### Bonus: upload the result to the cloud (Not graded)\n",
    "\n",
    "Just printing the mean duration inside the docker image \n",
    "doesn't seem very practical. Typically, after creating the output \n",
    "file, we upload it to the cloud storage.\n",
    "\n",
    "Modify your code to upload the parquet file to S3/GCS/etc.\n",
    "\n",
    "\n",
    "#### Bonus: Use an orchestrator for batch inference\n",
    "\n",
    "Here we didn't use any orchestration. In practice we usually do.\n",
    "\n",
    "* Split the code into logical code blocks\n",
    "* Use a workflow orchestrator for the code execution\n",
    "\n",
    "#### Publishing the image to dockerhub\n",
    "\n",
    "This is how we published the image to Docker hub:\n",
    "\n",
    "```bash\n",
    "docker build -t mlops-zoomcamp-model:2024-3.10.13-slim .\n",
    "docker tag mlops-zoomcamp-model:2024-3.10.13-slim agrigorev/zoomcamp-model:mlops-2024-3.10.13-slim\n",
    "\n",
    "docker login --username USERNAME\n",
    "docker push agrigorev/zoomcamp-model:mlops-2024-3.10.13-slim\n",
    "```\n",
    "\n",
    "This is just for your reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b6a55-6f22-44d6-b5ff-962e6f540f9a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
